<html>
<head>
    <meta charset="utf-8"/>
<meta name="description" content=""/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Kubernetes的介绍、基本概念和集群安装部署 | nobuggeek</title>
<link rel="shortcut icon" href="https://liuhuipy.github.io/favicon.ico?v=1591975563996">
<link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.css" rel="stylesheet">
<link rel="stylesheet" href="https://liuhuipy.github.io/styles/main.css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
      integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

<script src="https://cdn.bootcss.com/highlight.js/9.15.10/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/go.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
        crossorigin="anonymous"></script>

<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            nobuggeek
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
                <div class="nav-item">
                    
                        <a href="/" class="menu gt-a-link">
                            首页
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/archives" class="menu gt-a-link">
                            归档
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/tags" class="menu gt-a-link">
                            标签
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/about" class="menu gt-a-link">
                            关于
                        </a>
                    
                </div>
            
        </div>
    </div>
</nav>
    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    Kubernetes的介绍、基本概念和集群安装部署
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2020-06-09 ·
                    </time>
                    
                        <a href="https://liuhuipy.github.io/tag/1BPzjQv9Z/" class="post-tags">
                            # Devops
                        </a>
                    
                        <a href="https://liuhuipy.github.io/tag/DCBrCTQ9Z/" class="post-tags">
                            # Kubernetes
                        </a>
                    
                        <a href="https://liuhuipy.github.io/tag/CP6_bOFZEr/" class="post-tags">
                            # Docker
                        </a>
                    
                </div>
                <div class="post-content">
                    <p>Kubernetes简介、集群部署、基本概念</p>
<!-- more -->
<p>Kubernetes是一个全新的基于容器技术的分布式架构解决方案，是谷歌十几年以来大规模应用容器技术的经验积累和升华的重要成果。Kubernetes源自于谷歌一个叫Borg的内部容器管理系统（后来还有一个新系统叫Omega），谷歌部署Borg系统对来自于成千上万个应用程序所提交的job进行接收、调试、启动、停止、重启和监控，它基于容器技术，目的是实现资源管理的自动化，以及跨多个数据中心资源利用率的最大化。而横空出世的Kubernetes项目正是提取了Borg最精华的部分，使开发者能够更简单、直接地管理应用。</p>
<p>Kubernetes是一个完备的分布式系统支撑平台。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、智能的负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。</p>
<h2 id="kubernetes的核心功能">🍀Kubernetes的核心功能：</h2>
<ul>
<li>帮助开发者聚焦核心应用功能：Kubernetes可以被看作一个操作系统，应用所需要的基础设施不用开发者来担心。他们现在依赖于Kubernetes来提供这些服务，包括服务发现、扩容、负载均衡、自恢复。应用程序开发者因此能集中实现应用本身的功能而不用花时间在集成应用和基础设施上。</li>
<li>帮助运维团队获取更高的服务器资源利用率：Kubernetes自动调度应用容器到集群的某个节点上，Kubernetes能在任何时间迁移应用并通过混合和匹配应用来获得比运维手动调度高很多的资源利用率。</li>
</ul>
<h2 id="kubernetes安装与配置">🌳Kubernetes安装与配置</h2>
<h3 id="kubernetes集群搭建方式">Kubernetes集群搭建方式：</h3>
<ul>
<li>kubeadm：Kubernetes 1.4开始新增的特性，用于快速搭建Kubernetes集群；</li>
<li>minikube：快速搭建一个运行在本地的单节点的Kubernetes；</li>
<li>二进制安装：下载Kubernetes所需组件的二进制包，一步步安装。</li>
</ul>
<p>🍏为了更好的了解、熟悉Kubernetes集群的各个组件，本文采用二进制部署Kubernetes集群。</p>
<h3 id="集群详情">集群详情</h3>
<ul>
<li>OS：Centos Linux version 3.10.0-1127.el7.x86_64</li>
<li>Kubernetes 1.16.0</li>
<li>Docker：docker-1.13.1-161.git64e9980.el7_8.x86_64</li>
<li>Etcd 3.1.5</li>
<li>Flannel 0.7.1</li>
<li>TLS认证通信</li>
<li>RBAC授权</li>
<li>kubelet TLS Bootstrapping</li>
<li>kubedns、dashboard、heapster（influxdb、grafana）、EFK（elasticsearch、fluentd、kibana）集群插件</li>
<li>私有化docker镜像仓库harbor</li>
</ul>
<h3 id="环境说明">环境说明</h3>
<ul>
<li>镜像仓库：IP：192.168.143.133，Domain：harbor.liuhui.io</li>
<li>Etcd：192.168.143.128，192.168.143.129，192.168.143.130高可用etcd集群</li>
<li>Master：192.168.143.128，192.168.143.129，192.168.143.130三台，高可用</li>
<li>Node：192.168.143.128，192.168.143.129，192.168.143.130，192.168.143.131，192.168.143.122五台Node节点</li>
</ul>
<h3 id="安装前准备">安装前准备</h3>
<ul>
<li>环境说明</li>
<li>每个节点上安装Docker</li>
</ul>
<pre><code>yum install docker -y
cd /etc/docker
vi daemon.json
{
  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]
}
systemctl daemon-reload
systemctl start docker
</code></pre>
<ul>
<li>关闭所有节点的Selinux</li>
</ul>
<pre><code>vi /etc/selinux/config进入文件中修改配置SELINUX=disabled，然后重启服务器。
</code></pre>
<ul>
<li>准备harbor私有镜像仓库</li>
</ul>
<h3 id="创建tls证书和秘钥">创建TLS证书和秘钥</h3>
<p>所有操作都在Master节点192.168.143.128上执行，证书生成后拷贝到/etc/kubernetes目录下并拷贝到其他节点上的同样目录。</p>
<h4 id="生成的ca证书和秘钥文件如下">生成的CA证书和秘钥文件如下：</h4>
<ul>
<li>ca-key.pm</li>
<li>ca.pem</li>
<li>kubernetes-key.pem</li>
<li>kubernetes.pem</li>
<li>kube-proxy.pem</li>
<li>kube-proxy-key.pem</li>
<li>admin.pem</li>
<li>admin-key.pem</li>
</ul>
<h4 id="使用证书的组件如下">使用证书的组件如下：</h4>
<ul>
<li>etcd：使用ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kube-apiserver：使用ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kubelet：使用ca.pem；</li>
<li>kube-proxy：使用ca.pem、kube-proxy-key.pem、kube-proxy.pem；</li>
<li>kubectl：使用ca.pem、admin-key.pem、admin、pem；、</li>
<li>kube-controller-manager：使用ca-key.pem、ca、pem。</li>
</ul>
<h4 id="安装cfssl">安装CFSSL</h4>
<p>执行以下命令：</p>
<pre><code>wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
chmod +x cfssl_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssljson_linux-amd64
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
</code></pre>
<h4 id="创建cacertificate-authority">创建CA（Certificate Authority）</h4>
<p>创建CA配置文件：</p>
<pre><code>mkdir /root/ssl
cd /root/ssl
cfssl print-defaults config &gt; config.json
cfssl print-defaults csr &gt; csr.json

cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
EOF
</code></pre>
<ul>
<li>ca-config.json：可以定义多个profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个profile；</li>
<li>signing：表示该证书可用于签名其它证书，生成的ca.pem证书中CA=TRUE；</li>
<li>server auth：表示client可以用该CA对server提供的证书进行验证；</li>
<li>client auth：表示server可以用该CA对client提供的证书进行验证。</li>
</ul>
<p>创建CA证书签名请求：</p>
<pre><code>cat &gt; ca-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ],
    &quot;ca&quot;: {
       &quot;expiry&quot;: &quot;87600h&quot;
    }
}
EOF
</code></pre>
<ul>
<li>CN：Common Name，kube-apiserver从证书中提取该字段作为请求的用户名（User Name）；</li>
<li>O：Organization，kube-apiserver从证书中提取该字段作为请求用户所属的组（Group）；</li>
</ul>
<p>生成CA证书和私钥：</p>
<pre><code># cfssl gencert -initca ca-csr.json | cfssljson -bare ca
# ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
</code></pre>
<h4 id="创建kubernetes证书">创建Kubernetes证书</h4>
<p>创建kubernetes证书签名请求文件kubernetes-csr.json：</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;172.20.0.128&quot;,
    &quot;192.168.143.128&quot;,
    &quot;192.168.143.129&quot;,
    &quot;192.168.143.130&quot;,
    &quot;192.168.143.131&quot;,
    &quot;192.168.143.132&quot;,
    &quot;10.254.0.1&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre>
<ul>
<li>如果hosts字段不为空则需要指定授权使用该证书的IP地址或者域名列表，由于该证书后续被etcd集群和kubernetes master集群使用，所以上面分别指定了etcd集群、kubernetes master集群和Kubernetes Node节点的IP地址；</li>
</ul>
<p>生成Kubernetes证书和私钥：</p>
<pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
# ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
</code></pre>
<h4 id="创建admin证书">创建admin证书</h4>
<p>创建admin证书签名请求文件admin-csr.json：</p>
<pre><code>{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre>
<ul>
<li>后续kube-apiserver使用RBAC对客户端（如kubelet、kube-proxy、Pod）请求进行授权；</li>
<li>kube-apiserver预定义了一些RBAC的RoleBindings，如cluster-admin将Group system:masters与Role cluster-admin绑定，该Role授予了调用kube-apiserver的所有API的权限；</li>
<li>O指定该证书的Group为system:masters，kubelet使用该证书访问kube-apiserver时，由于证书被CA签名，所以认证通过，同时由于证书用户组为经过预授权的system:masters，所以被授权访问所有API的权限。</li>
</ul>
<p>生成admin证书和私钥：</p>
<pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
# ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
</code></pre>
<h4 id="创建kube-proxy证书">创建kube-proxy证书</h4>
<p>创建kube-proxy证书签名请求文件kube-proxy-csr.json：</p>
<pre><code>{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre>
<ul>
<li>CN指定该证书的User为system:kube-proxy；</li>
<li>kube-apiserver预定义的RoleBinding system:node-proxier将User system:kube-proxy与Role system:node-proxier绑定，该Role授予了调用kube-apiserver Proxy相关API的权限。</li>
</ul>
<p>生成kube-proxy客户端证书和私钥：</p>
<pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
# ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
</code></pre>
<p>使用openssl命令校验证书：</p>
<pre><code>openssl x509  -noout -text -in  kubernetes.pem
</code></pre>
<ul>
<li>确认Issuer字段的内容和ca-csr.json一致；</li>
<li>确认Subject字段的内容和kubernetes-csr.json一致；</li>
<li>确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；</li>
<li>确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致。</li>
</ul>
<h4 id="分发证书">分发证书</h4>
<p>将生成的证书和秘钥文件拷贝到所有节点的/etc/kubernetes/ssl（先给每个节点mkdir创建该目录）目录下：</p>
<pre><code>mkdir -p /etc/kubernetes/ssl
cp *.pem /etc/kubernetes/ssl
scp *.pem root@192.168.143.129:/etc/kubernetes/ssl
scp *.pem root@192.168.143.130:/etc/kubernetes/ssl
scp *.pem root@192.168.143.131:/etc/kubernetes/ssl
scp *.pem root@192.168.143.132:/etc/kubernetes/ssl
</code></pre>
<h3 id="安装kubectl与创建kubeconfig文件">安装kubectl与创建kubeconfig文件</h3>
<h4 id="下载kubectl">下载kubectl</h4>
<pre><code>wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz
cp kubernetes/client/bin/kube* /usr/bin/
chmod a+x /usr/bin/kube*
</code></pre>
<h4 id="创建kubectl-kubeconfig文件">创建kubectl kubeconfig文件</h4>
<pre><code>export KUBE_APISERVER=&quot;https://192.168.143.128:6443&quot;
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER}
# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem
# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin
# 设置默认上下文
kubectl config use-context kubernetes
</code></pre>
<h4 id="创建tls-bootstrapping-token">创建TLS Bootstrapping Token</h4>
<p>生成token auth file，Token可以是任意的包含128 bit的字符串，可以使用安全的随机数生成器生成：</p>
<pre><code>export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
cat &gt; token.csv &lt;&lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;
EOF
cp token.csv /etc/kubernetes/
</code></pre>
<h4 id="创建kubelet-bootstrapping-kubeconfig文件">创建kubelet bootstrapping kubeconfig文件</h4>
<pre><code>cd /etc/kubernetes
export KUBE_APISERVER=&quot;https://192.168.143.128:6443&quot;

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
</code></pre>
<h4 id="创建kube-proxy-kubeconfig文件">创建kube-proxy kubeconfig文件</h4>
<pre><code>export KUBE_APISERVER=&quot;https://192.168.143.128:6443&quot;
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
</code></pre>
<h4 id="分发kubeconfig文件">分发kubeconfig文件</h4>
<p>将两个kubeconfig文件分发到所有Node节点的/etc/kubernetes/目录下：</p>
<pre><code>scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.129:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.130:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.131:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.132:/etc/kubernetes/
</code></pre>
<h3 id="创建高可用etcd集群">创建高可用etcd集群</h3>
<p>kubernetes系统使用etcd存储所有数据，部署三个节点高可用etcd集群，复用kubernetes master的节点：</p>
<ul>
<li>192.168.143.128</li>
<li>192.168.143.129</li>
<li>192.168.143.130</li>
</ul>
<h4 id="tls认证文件">TLS认证文件</h4>
<p>这里复用kubernetes master创建的kubernetes证书</p>
<pre><code>scp ca.pem kubernetes-key.pem kubernetes.pem root@192.168.143.129:/etc/kubernetes/ssl
scp ca.pem kubernetes-key.pem kubernetes.pem root@192.168.143.130:/etc/kubernetes/ssl
</code></pre>
<h4 id="安装etcd">安装etcd</h4>
<p>三个节点都需要以下操作。<br>
下载二进制文件：</p>
<pre><code>wget https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz
tar -xvf etcd-v3.1.5-linux-amd64.tar.gz
mv etcd-v3.1.5-linux-amd64/etcd* /usr/local/bin
</code></pre>
<p>在/usr/lib/systemd/system/目录下创建etcd的systemd unit文件etcd.service：</p>
<pre><code>[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
  --listen-peer-urls ${ETCD_LISTEN_PEER_URLS} \
  --listen-client-urls ${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
  --advertise-client-urls ${ETCD_ADVERTISE_CLIENT_URLS} \
  --initial-cluster-token ${ETCD_INITIAL_CLUSTER_TOKEN} \
  --initial-cluster infra1=https://192.168.143.128:2380,infra2=https://192.168.143.129:2380,infra3=https://192.168.143.130:2380 \
  --initial-cluster-state new \
  --data-dir=${ETCD_DATA_DIR}
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<ul>
<li>指定etcd的工作目录为/var/lib/etcd，数据目录为/var/lib/etcd，启动服务前得创建该目录；</li>
<li>为了保证通信安全，需要指定etcd的公私钥（cert-file和key-file）、Peers通信的公私钥和CA证书（peer-cert-file、peer-key-file、peer=trusted-ca-file）、客户端的CA证书（trusted-ca-file）；</li>
<li>创建kubernetes.pem证书时使用的kubernetes-csr.json文件的hosts字段包含所有etcd节点的IP，否则证书校验会出错；</li>
<li>--initial-cluster-state值为new时，--name的参数值必须位于--initial-cluster列表中。</li>
</ul>
<p>环境变量配置文件/etc/etcd/etcd.conf（这里注意每个etcd节点ETCD_NAME和IP要进行更改）：</p>
<pre><code># [member]
ETCD_NAME=infra1
ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;
ETCD_LISTEN_PEER_URLS=&quot;https://192.168.143.128:2380&quot;
ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.143.128:2379&quot;

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.143.128:2380&quot;
ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;
ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.143.128:2379&quot;
</code></pre>
<h4 id="启动etcd服务">启动etcd服务</h4>
<pre><code>systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
</code></pre>
<p>为了确保所有节点防火墙开放了2379、2380端口：</p>
<pre><code>firewall-cmd --zone=public --add-port=2380/tcp --permanent
firewall-cmd --zone=public --add-port=2379/tcp --permanent
firewall-cmd --reload
</code></pre>
<h4 id="验证etcd服务">验证etcd服务</h4>
<p>在任意etcd节点上执行：</p>
<pre><code># etcdctl   --ca-file=/etc/kubernetes/ssl/ca.pem   --cert-file=/etc/kubernetes/ssl/kubernetes.pem   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem   cluster-health

2020-06-10 02:04:18.318845 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
2020-06-10 02:04:18.319487 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
member 3624da61eeb2a34b is healthy: got healthy result from https://192.168.143.129:2379
member 83f50994fc838c06 is healthy: got healthy result from https://192.168.143.128:2379
member c507e67572829987 is healthy: got healthy result from https://192.168.143.130:2379
cluster is healthy
</code></pre>
<h3 id="部署master节点">部署master节点</h3>
<p>Kubernetes master节点包含的组件：</p>
<ul>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kube-controller-manager</li>
</ul>
<h4 id="下载对应的kubernetes版本">下载对应的Kubernetes版本</h4>
<pre><code>wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
</code></pre>
<h4 id="配置kube-apiserver-kube-scheduler-kube-controller-manager文件">配置kube-apiserver、kube-scheduler、kube-controller-manager文件</h4>
<p>添加/etc/kubernetes/config文件：</p>
<pre><code>###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&quot;--v=0&quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER=&quot;--master=http://192.168.143.128:8080&quot;
</code></pre>
<p>该配置文件同时被kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy使用。</p>
<p>添加apiserver配置文件/etc/kubernetes/apiserver：</p>
<pre><code>###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
KUBE_API_ADDRESS=&quot;--advertise-address=192.168.143.128 --bind-address=192.168.143.128 --insecure-bind-address=192.168.143.128&quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&quot;--port=8080&quot;
#
## Port minions listen on
#KUBELET_PORT=&quot;--kubelet-port=10250&quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.143.128:2379,https://192.168.143.129:2379,https://192.168.143.130:2379&quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&quot;
#
## Add your own!
KUBE_API_ARGS=&quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&quot;
</code></pre>
<p>添加scheduler配置文件/etc/kubernetes/scheduler：</p>
<pre><code>###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot;
</code></pre>
<p>添加controller-manager配置文件/etc/kubernetes/controller-manager：</p>
<pre><code>###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&quot;
</code></pre>
<h4 id="创建kube-apiserver-service配置文件和启动kube-apiserver">创建kube-apiserver service配置文件和启动kube-apiserver</h4>
<p>创建kube-apiserver的service配置文件/usr/lib/systemd/system/kube-apiserver.service：</p>
<pre><code>[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<p>启动kube-apiserver</p>
<pre><code>systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
</code></pre>
<h4 id="创建kube-scheduler-service文件和启动kube-scheduler">创建kube-scheduler service文件和启动kube-scheduler</h4>
<p>创建kube-scheduler的service配置文件/usr/lib/systemd/system/kube-scheduler.service：</p>
<pre><code>[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<p>启动kube-scheduler</p>
<pre><code>systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler
</code></pre>
<h4 id="创建kube-controller-manager-service文件和启动kube-controller-manager">创建kube-controller-manager service文件和启动kube-controller-manager</h4>
<p>创建kube-controller-manager的service配置文件/usr/lib/systemd/kube-controller-manager.service：</p>
<pre><code>[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_MASTER \
        $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<p>启动kube-controller-manager</p>
<pre><code>systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
</code></pre>
<h4 id="验证master节点功能">验证master节点功能</h4>
<pre><code># kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}
</code></pre>
<h3 id="部署node节点">部署node节点</h3>
<h3 id="安装dashboard">安装dashboard</h3>
<h2 id="kubernetes基本概念">🌴Kubernetes基本概念</h2>
<p>Kubernetes集群架构：<br>
<img src="https://liuhuipy.github.io/post-images/1591687453392.png" alt="" loading="lazy"></p>
<h3 id="master">Master</h3>
<p>Kubernetes采用Master作为集群控制中心节点，每个Kubernetes集群里需要有一个Master节点来负责整个集群的管理和控制，Master节点上运行着一组进程：</p>
<ul>
<li>Kubernetes API Server（kube-apiserver）：提供了HTTP Rest接口的关键服务进程，是Kubernetes所有资源的增、删、改、查等操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制，是集群控制的入口进程。</li>
<li>Kubernetes Controller Manager（kube-controller-manager）：Kubernetes所有资源对象的自动化控制中心，负责维护集群的状态，比如故障检测、自动扩展、滚动更新等。</li>
<li>Kubernetes Scheduler（kube-scheduler）：负责资源调度（Pod调度）的进程。</li>
</ul>
<h3 id="node">Node</h3>
<p>Kubernetes集群中除了Master节点的其他节点称为Node节点。与Master一样，Node节点可以是一台物理主机，也可以是一台虚拟机。Node节点才是Kubernetes集群中的工作负载节点，每个Node都会被分配一些工作负载(Pod)，当某个Node宕机时，这些负载会被分配到其他工作节点中。每个Node节点上都运行着以下一组关键进程：</p>
<ul>
<li>kubelet：负责Pod对应的容器的创建、启停等任务。</li>
<li>kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件。</li>
<li>Docker Engine（Docker）：Docker容器引擎，负责工作节点的容器创建和管理工作。</li>
</ul>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://liuhuipy.github.io/post/kubernetes-node/" class="post-title gt-a-link">
                    Kubernetes学习笔记
                </a>
            </div>
        

        

        
            
                <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '18b5ef05a5b9a900a0e1',
    clientSecret: '8c945f07ebb35fcceae898fcbfc45c9ff92042fe',
    repo: 'liuhuipy.github.io',
    owner: 'liuhuipy',
    admin: ['liuhuipy'],
    id: location.pathname,      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

            

            
        

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">温故而知新</div>
    <div class="social-container">
        
            
                <a href="https://github.com/liuhuipy" target="_blank">
                    <i class="fab fa-github gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/liuhuipy" target="_blank">nobuggeek</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://liuhuipy.github.io/atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
    hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
