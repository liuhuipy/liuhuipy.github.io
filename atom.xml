<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://liuhuipy.github.io</id>
    <title>nobuggeek</title>
    <updated>2020-06-10T09:33:36.295Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://liuhuipy.github.io"/>
    <link rel="self" href="https://liuhuipy.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://liuhuipy.github.io/images/avatar.png</logo>
    <icon>https://liuhuipy.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, nobuggeek</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes的介绍、基本概念和集群安装部署]]></title>
        <id>https://liuhuipy.github.io/post/kubernetes-Introduction/</id>
        <link href="https://liuhuipy.github.io/post/kubernetes-Introduction/">
        </link>
        <updated>2020-06-09T03:17:39.000Z</updated>
        <summary type="html"><![CDATA[<p>Kubernetes简介、集群部署、基本概念</p>
]]></summary>
        <content type="html"><![CDATA[<p>Kubernetes简介、集群部署、基本概念</p>
<!-- more -->
<p>Kubernetes是一个全新的基于容器技术的分布式架构解决方案，是谷歌十几年以来大规模应用容器技术的经验积累和升华的重要成果。Kubernetes源自于谷歌一个叫Borg的内部容器管理系统（后来还有一个新系统叫Omega），谷歌部署Borg系统对来自于成千上万个应用程序所提交的job进行接收、调试、启动、停止、重启和监控，它基于容器技术，目的是实现资源管理的自动化，以及跨多个数据中心资源利用率的最大化。而横空出世的Kubernetes项目正是提取了Borg最精华的部分，使开发者能够更简单、直接地管理应用。</p>
<p>Kubernetes是一个完备的分布式系统支撑平台。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、智能的负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。</p>
<h2 id="kubernetes的核心功能">🍀Kubernetes的核心功能：</h2>
<ul>
<li>帮助开发者聚焦核心应用功能：Kubernetes可以被看作一个操作系统，应用所需要的基础设施不用开发者来担心。他们现在依赖于Kubernetes来提供这些服务，包括服务发现、扩容、负载均衡、自恢复。应用程序开发者因此能集中实现应用本身的功能而不用花时间在集成应用和基础设施上。</li>
<li>帮助运维团队获取更高的服务器资源利用率：Kubernetes自动调度应用容器到集群的某个节点上，Kubernetes能在任何时间迁移应用并通过混合和匹配应用来获得比运维手动调度高很多的资源利用率。</li>
</ul>
<h2 id="kubernetes安装与配置">🌳Kubernetes安装与配置</h2>
<h3 id="kubernetes集群搭建方式">Kubernetes集群搭建方式：</h3>
<ul>
<li>kubeadm：Kubernetes 1.4开始新增的特性，用于快速搭建Kubernetes集群；</li>
<li>minikube：快速搭建一个运行在本地的单节点的Kubernetes；</li>
<li>二进制安装：下载Kubernetes所需组件的二进制包，一步步安装。</li>
</ul>
<p>🍏为了更好的了解、熟悉Kubernetes集群的各个组件，本文采用二进制部署Kubernetes集群。</p>
<h3 id="集群详情">集群详情</h3>
<ul>
<li>OS：Centos Linux version 3.10.0-1127.el7.x86_64</li>
<li>Kubernetes 1.18.0</li>
<li>Docker：docker-1.13.1-161.git64e9980.el7_8.x86_64</li>
<li>Etcd 3.1.5</li>
<li>Flannel 0.7.1</li>
<li>TLS认证通信</li>
<li>RBAC授权</li>
<li>kubelet TLS Bootstrapping</li>
<li>kubedns、dashboard、heapster（influxdb、grafana）、EFK（elasticsearch、fluentd、kibana）集群插件</li>
<li>私有化docker镜像仓库harbor</li>
</ul>
<h3 id="环境说明">环境说明</h3>
<ul>
<li>镜像仓库：IP：192.168.143.133，Domain：harbor.liuhui.io</li>
<li>Etcd：192.168.143.128，192.168.143.129，192.168.143.130高可用etcd集群</li>
<li>Master：192.168.143.128，192.168.143.129，192.168.143.130三台，高可用</li>
<li>Node：192.168.143.128，192.168.143.129，192.168.143.130，192.168.143.131，192.168.143.122五台Node节点</li>
</ul>
<h3 id="安装前准备">安装前准备</h3>
<ul>
<li>环境说明</li>
<li>每个节点上安装Docker</li>
</ul>
<pre><code>yum install docker -y
cd /etc/docker
vi daemon.json
{
  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]
}
systemctl daemon-reload
systemctl start docker
</code></pre>
<ul>
<li>关闭所有节点的Selinux</li>
</ul>
<pre><code>vi /etc/selinux/config进入文件中修改配置SELINUX=disabled，然后重启服务器。
</code></pre>
<ul>
<li>准备harbor私有镜像仓库</li>
</ul>
<h3 id="创建tls证书和秘钥">创建TLS证书和秘钥</h3>
<p>所有操作都在Master节点192.168.143.128上执行，证书生成后拷贝到/etc/kubernetes目录下并拷贝到其他节点上的同样目录。</p>
<h4 id="生成的ca证书和秘钥文件如下">生成的CA证书和秘钥文件如下：</h4>
<ul>
<li>ca-key.pm</li>
<li>ca.pem</li>
<li>kubernetes-key.pem</li>
<li>kubernetes.pem</li>
<li>kube-proxy.pem</li>
<li>kube-proxy-key.pem</li>
<li>admin.pem</li>
<li>admin-key.pem</li>
</ul>
<h4 id="使用证书的组件如下">使用证书的组件如下：</h4>
<ul>
<li>etcd：使用ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kube-apiserver：使用ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kubelet：使用ca.pem；</li>
<li>kube-proxy：使用ca.pem、kube-proxy-key.pem、kube-proxy.pem；</li>
<li>kubectl：使用ca.pem、admin-key.pem、admin、pem；、</li>
<li>kube-controller-manager：使用ca-key.pem、ca、pem。</li>
</ul>
<h4 id="安装cfssl">安装CFSSL</h4>
<p>执行以下命令：</p>
<pre><code>wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
chmod +x cfssl_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssljson_linux-amd64
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
</code></pre>
<h4 id="创建cacertificate-authority">创建CA（Certificate Authority）</h4>
<p>创建CA配置文件：</p>
<pre><code>mkdir /root/ssl
cd /root/ssl
cfssl print-defaults config &gt; config.json
cfssl print-defaults csr &gt; csr.json

cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
EOF
</code></pre>
<ul>
<li>ca-config.json：可以定义多个profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个profile；</li>
<li>signing：表示该证书可用于签名其它证书，生成的ca.pem证书中CA=TRUE；</li>
<li>server auth：表示client可以用该CA对server提供的证书进行验证；</li>
<li>client auth：表示server可以用该CA对client提供的证书进行验证。</li>
</ul>
<p>创建CA证书签名请求：</p>
<pre><code>cat &gt; ca-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ],
    &quot;ca&quot;: {
       &quot;expiry&quot;: &quot;87600h&quot;
    }
}
EOF
</code></pre>
<ul>
<li>CN：Common Name，kube-apiserver从证书中提取该字段作为请求的用户名（User Name）；</li>
<li>O：Organization，kube-apiserver从证书中提取该字段作为请求用户所属的组（Group）；</li>
</ul>
<p>生成CA证书和私钥：</p>
<pre><code># cfssl gencert -initca ca-csr.json | cfssljson -bare ca
# ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
</code></pre>
<h4 id="创建kubernetes证书">创建Kubernetes证书</h4>
<p>创建kubernetes证书签名请求文件kubernetes-csr.json：</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;172.20.0.128&quot;,
    &quot;192.168.143.128&quot;,
    &quot;192.168.143.129&quot;,
    &quot;192.168.143.130&quot;,
    &quot;192.168.143.131&quot;,
    &quot;192.168.143.132&quot;,
    &quot;10.254.0.1&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre>
<ul>
<li>如果hosts字段不为空则需要指定授权使用该证书的IP地址或者域名列表，由于该证书后续被etcd集群和kubernetes master集群使用，所以上面分别指定了etcd集群、kubernetes master集群和Kubernetes Node节点的IP地址；</li>
</ul>
<p>生成Kubernetes证书和私钥：</p>
<pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
# ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
</code></pre>
<h4 id="创建admin证书">创建admin证书</h4>
<p>创建admin证书签名请求文件admin-csr.json：</p>
<pre><code>{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre>
<ul>
<li>后续kube-apiserver使用RBAC对客户端（如kubelet、kube-proxy、Pod）请求进行授权；</li>
<li>kube-apiserver预定义了一些RBAC的RoleBindings，如cluster-admin将Group system:masters与Role cluster-admin绑定，该Role授予了调用kube-apiserver的所有API的权限；</li>
<li>O指定该证书的Group为system:masters，kubelet使用该证书访问kube-apiserver时，由于证书被CA签名，所以认证通过，同时由于证书用户组为经过预授权的system:masters，所以被授权访问所有API的权限。</li>
</ul>
<p>生成admin证书和私钥：</p>
<pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
# ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
</code></pre>
<h4 id="创建kube-proxy证书">创建kube-proxy证书</h4>
<p>创建kube-proxy证书签名请求文件kube-proxy-csr.json：</p>
<pre><code>{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre>
<ul>
<li>CN指定该证书的User为system:kube-proxy；</li>
<li>kube-apiserver预定义的RoleBinding system:node-proxier将User system:kube-proxy与Role system:node-proxier绑定，该Role授予了调用kube-apiserver Proxy相关API的权限。</li>
</ul>
<p>生成kube-proxy客户端证书和私钥：</p>
<pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
# ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
</code></pre>
<p>使用openssl命令校验证书：</p>
<pre><code>openssl x509  -noout -text -in  kubernetes.pem
</code></pre>
<ul>
<li>确认Issuer字段的内容和ca-csr.json一致；</li>
<li>确认Subject字段的内容和kubernetes-csr.json一致；</li>
<li>确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；</li>
<li>确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致。</li>
</ul>
<h4 id="分发证书">分发证书</h4>
<p>将生成的证书和秘钥文件拷贝到所有节点的/etc/kubernetes/ssl（先给每个节点mkdir创建该目录）目录下：</p>
<pre><code>mkdir -p /etc/kubernetes/ssl
cp *.pem /etc/kubernetes/ssl
scp *.pem root@192.168.143.129:/etc/kubernetes/ssl
scp *.pem root@192.168.143.130:/etc/kubernetes/ssl
scp *.pem root@192.168.143.131:/etc/kubernetes/ssl
scp *.pem root@192.168.143.132:/etc/kubernetes/ssl
</code></pre>
<h3 id="安装kubectl与创建kubeconfig文件">安装kubectl与创建kubeconfig文件</h3>
<h4 id="下载kubectl">下载kubectl</h4>
<pre><code>wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz
cp kubernetes/client/bin/kube* /usr/bin/
chmod a+x /usr/bin/kube*
</code></pre>
<h4 id="创建kubectl-kubeconfig文件">创建kubectl kubeconfig文件</h4>
<pre><code>export KUBE_APISERVER=&quot;https://192.168.143.128:6443&quot;
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER}
# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem
# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin
# 设置默认上下文
kubectl config use-context kubernetes
</code></pre>
<h4 id="创建tls-bootstrapping-token">创建TLS Bootstrapping Token</h4>
<p>生成token auth file，Token可以是任意的包含128 bit的字符串，可以使用安全的随机数生成器生成：</p>
<pre><code>export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
cat &gt; token.csv &lt;&lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;
EOF
cp token.csv /etc/kubernetes/
</code></pre>
<h4 id="创建kubelet-bootstrapping-kubeconfig文件">创建kubelet bootstrapping kubeconfig文件</h4>
<pre><code>cd /etc/kubernetes
export KUBE_APISERVER=&quot;https://192.168.143.128:6443&quot;

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
</code></pre>
<h4 id="创建kube-proxy-kubeconfig文件">创建kube-proxy kubeconfig文件</h4>
<pre><code>export KUBE_APISERVER=&quot;https://192.168.143.128:6443&quot;
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
</code></pre>
<h4 id="分发kubeconfig文件">分发kubeconfig文件</h4>
<p>将两个kubeconfig文件分发到所有Node节点的/etc/kubernetes/目录下：</p>
<pre><code>scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.129:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.130:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.131:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.143.132:/etc/kubernetes/
</code></pre>
<h3 id="创建高可用etcd集群">创建高可用etcd集群</h3>
<p>kubernetes系统使用etcd存储所有数据，部署三个节点高可用etcd集群，复用kubernetes master的节点：</p>
<ul>
<li>192.168.143.128</li>
<li>192.168.143.129</li>
<li>192.168.143.130</li>
</ul>
<h4 id="tls认证文件">TLS认证文件</h4>
<p>这里复用kubernetes master创建的kubernetes证书</p>
<pre><code>scp ca.pem kubernetes-key.pem kubernetes.pem root@192.168.143.129:/etc/kubernetes/ssl
scp ca.pem kubernetes-key.pem kubernetes.pem root@192.168.143.130:/etc/kubernetes/ssl
</code></pre>
<h4 id="安装etcd">安装etcd</h4>
<p>三个节点都需要以下操作。<br>
下载二进制文件：</p>
<pre><code>wget https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz
tar -xvf etcd-v3.1.5-linux-amd64.tar.gz
mv etcd-v3.1.5-linux-amd64/etcd* /usr/local/bin
</code></pre>
<p>在/usr/lib/systemd/system/目录下创建etcd的systemd unit文件etcd.service：</p>
<pre><code>[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
  --listen-peer-urls ${ETCD_LISTEN_PEER_URLS} \
  --listen-client-urls ${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
  --advertise-client-urls ${ETCD_ADVERTISE_CLIENT_URLS} \
  --initial-cluster-token ${ETCD_INITIAL_CLUSTER_TOKEN} \
  --initial-cluster infra1=https://192.168.143.128:2380,infra2=https://192.168.143.129:2380,infra3=https://192.168.143.130:2380 \
  --initial-cluster-state new \
  --data-dir=${ETCD_DATA_DIR}
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<ul>
<li>指定etcd的工作目录为/var/lib/etcd，数据目录为/var/lib/etcd，启动服务前得创建该目录；</li>
<li>为了保证通信安全，需要指定etcd的公私钥（cert-file和key-file）、Peers通信的公私钥和CA证书（peer-cert-file、peer-key-file、peer=trusted-ca-file）、客户端的CA证书（trusted-ca-file）；</li>
<li>创建kubernetes.pem证书时使用的kubernetes-csr.json文件的hosts字段包含所有etcd节点的IP，否则证书校验会出错；</li>
<li>--initial-cluster-state值为new时，--name的参数值必须位于--initial-cluster列表中。</li>
</ul>
<p>环境变量配置文件/etc/etcd/etcd.conf（这里注意每个etcd节点ETCD_NAME和IP要进行更改）：</p>
<pre><code># [member]
ETCD_NAME=infra1
ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;
ETCD_LISTEN_PEER_URLS=&quot;https://192.168.143.128:2380&quot;
ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.143.128:2379&quot;

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.143.128:2380&quot;
ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;
ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.143.128:2379&quot;
</code></pre>
<h4 id="启动etcd服务">启动etcd服务</h4>
<pre><code>systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
</code></pre>
<p>为了确保所有节点防火墙开放了2379、2380端口：</p>
<pre><code>firewall-cmd --zone=public --add-port=2380/tcp --permanent
firewall-cmd --zone=public --add-port=2379/tcp --permanent
firewall-cmd --reload
</code></pre>
<h4 id="验证etcd服务">验证etcd服务</h4>
<p>在任意etcd节点上执行：</p>
<pre><code># etcdctl   --ca-file=/etc/kubernetes/ssl/ca.pem   --cert-file=/etc/kubernetes/ssl/kubernetes.pem   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem   cluster-health

2020-06-10 02:04:18.318845 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
2020-06-10 02:04:18.319487 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
member 3624da61eeb2a34b is healthy: got healthy result from https://192.168.143.129:2379
member 83f50994fc838c06 is healthy: got healthy result from https://192.168.143.128:2379
member c507e67572829987 is healthy: got healthy result from https://192.168.143.130:2379
cluster is healthy
</code></pre>
<h3 id="部署master节点">部署master节点</h3>
<p>Kubernetes master节点包含的组件：</p>
<ul>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kube-controller-manager</li>
</ul>
<h4 id="下载对应的kubernetes版本">下载对应的Kubernetes版本</h4>
<pre><code>wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
</code></pre>
<h4 id="配置kube-apiserver-kube-scheduler-kube-controller-manager文件">配置kube-apiserver、kube-scheduler、kube-controller-manager文件</h4>
<p>添加/etc/kubernetes/config文件：</p>
<pre><code>###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&quot;--v=0&quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER=&quot;--master=http://192.168.143.128:8080&quot;
</code></pre>
<p>该配置文件同时被kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy使用。</p>
<p>添加apiserver配置文件/etc/kubernetes/apiserver：</p>
<pre><code>###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
KUBE_API_ADDRESS=&quot;--advertise-address=192.168.143.128 --bind-address=192.168.143.128 --insecure-bind-address=192.168.143.128&quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&quot;--port=8080&quot;
#
## Port minions listen on
#KUBELET_PORT=&quot;--kubelet-port=10250&quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.143.128:2379,https://192.168.143.129:2379,https://192.168.143.130:2379&quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&quot;
#
## Add your own!
KUBE_API_ARGS=&quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&quot;
</code></pre>
<p>添加scheduler配置文件/etc/kubernetes/scheduler：</p>
<pre><code>###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot;
</code></pre>
<p>添加controller-manager配置文件/etc/kubernetes/controller-manager：</p>
<pre><code>###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&quot;
</code></pre>
<h4 id="创建kube-apiserver-service配置文件和启动kube-apiserver">创建kube-apiserver service配置文件和启动kube-apiserver</h4>
<p>创建kube-apiserver的service配置文件/usr/lib/systemd/system/kube-apiserver.service：</p>
<pre><code>[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<p>启动kube-apiserver</p>
<pre><code>systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
</code></pre>
<h4 id="创建kube-scheduler-service文件和启动kube-scheduler">创建kube-scheduler service文件和启动kube-scheduler</h4>
<p>创建kube-scheduler的service配置文件/usr/lib/systemd/system/kube-scheduler.service：</p>
<pre><code>[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<p>启动kube-scheduler</p>
<pre><code>systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler
</code></pre>
<h4 id="创建kube-controller-manager-service文件和启动kube-controller-manager">创建kube-controller-manager service文件和启动kube-controller-manager</h4>
<p>创建kube-controller-manager的service配置文件/usr/lib/systemd/kube-controller-manager.service：</p>
<pre><code>[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_MASTER \
        $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<p>启动kube-controller-manager</p>
<pre><code>systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
</code></pre>
<h4 id="验证master节点功能">验证master节点功能</h4>
<pre><code># kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}
</code></pre>
<h2 id="kubernetes基本概念">🌴Kubernetes基本概念</h2>
<p>Kubernetes集群架构：<br>
<img src="https://liuhuipy.github.io/post-images/1591687453392.png" alt="" loading="lazy"></p>
<h3 id="master">Master</h3>
<p>Kubernetes采用Master作为集群控制中心节点，每个Kubernetes集群里需要有一个Master节点来负责整个集群的管理和控制，Master节点上运行着一组进程：</p>
<ul>
<li>Kubernetes API Server（kube-apiserver）：提供了HTTP Rest接口的关键服务进程，是Kubernetes所有资源的增、删、改、查等操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制，是集群控制的入口进程。</li>
<li>Kubernetes Controller Manager（kube-controller-manager）：Kubernetes所有资源对象的自动化控制中心，负责维护集群的状态，比如故障检测、自动扩展、滚动更新等。</li>
<li>Kubernetes Scheduler（kube-scheduler）：负责资源调度（Pod调度）的进程。</li>
</ul>
<h3 id="node">Node</h3>
<p>Kubernetes集群中除了Master节点的其他节点称为Node节点。与Master一样，Node节点可以是一台物理主机，也可以是一台虚拟机。Node节点才是Kubernetes集群中的工作负载节点，每个Node都会被分配一些工作负载(Pod)，当某个Node宕机时，这些负载会被分配到其他工作节点中。每个Node节点上都运行着以下一组关键进程：</p>
<ul>
<li>kubelet：负责Pod对应的容器的创建、启停等任务。</li>
<li>kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件。</li>
<li>Docker Engine（Docker）：Docker容器引擎，负责工作节点的容器创建和管理工作。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes学习笔记]]></title>
        <id>https://liuhuipy.github.io/post/kubernetes-node/</id>
        <link href="https://liuhuipy.github.io/post/kubernetes-node/">
        </link>
        <updated>2020-06-02T18:02:30.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Kubernetes</strong> ！<br>
✍️ Kubernetes权威指南笔记</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Kubernetes</strong> ！<br>
✍️ Kubernetes权威指南笔记</p>
<!-- more -->
<h1 id="kubernetes入门">Kubernetes入门</h1>
<h2 id="kubernetes是什么">Kubernetes是什么</h2>
<p>Kubernetes是一个完备的分布式系统支撑平台。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。</p>
<p>在Kubernetes中，Service（服务）是分布式集群架构的核心，一个Service对象拥有如下关键特征：</p>
<ul>
<li>拥有一个唯一指定的名字（比如mysql-server）；</li>
<li>拥有一个虚拟IP（Cluster IP、Service IP或VIP）和端口号；</li>
<li>能够提供某种远程服务能力；</li>
<li>被映射到了提供这种服务能力的一组容器应用上。</li>
</ul>
<p>Service的服务进程都基于Socket通信方式对外提供服务，比如Redis、Memcache、MySQL、WebServer，或者是实现了某个具体业务的一个特定的TCP Server进程。虽然一个Service通常由多个相关的进程来提供服务，每个服务进程都有一个独立的（IP+Port）访问点，但Kubernetes能够让我们通过Service（虚拟Cluster IP+Service Port）连接到指定的Service上。有了Kubernetes内建的透明负载均衡和故障恢复机制，不管后端有多少服务进程，也不管某个服务进程是否会由于发生故障而重新部署到其他机器，都不会影响到我们对服务的正常调用。更重要的是这个Service本身一旦创建就不再变化。</p>
<p>容器提供了强大的隔离功能，所以有必要把为Service提供服务的这组进程放入容器中进行隔离。为此，Kubernetes设计了Pod对象，将每个服务进程包装到相应的Pod中，使其成为Pod中运行的一个容器（Container）。为了建立Service和Pod间的关联关系，Kubernetes首先给每个Pod贴上一个标签（Label），给运行MySQL的Pod贴上name=mysql标签，给运行PHP的Pod贴上name=php标签，然后给相应的Service定义标签选择器（Label Selector），比如MySQL Service的标签选择器的选择条件为name=mysql，意味着该Service要作用于所有包含name=mysql Label的Pod上。</p>
<p>Pod运行在一个我们称为节点（Node）的环境中，这个节点既可以是物理机，也可以是虚拟机，通常一个节点上运行几百个Pod；其次，每个Pod里运行着一个特殊的被称之为Pause的容器，其他容器则为业务容器，这些业务容器共享Pause容器的网络栈和Volume挂载卷，因此它们之间通信和数据交换更为高效。</p>
<p>在集群管理方面，Kubernetes将集群中的机器划分为一个Master节点和一群工作节点（Node）。其中，在Master节点上运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler，这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理功能，并且都是全自动完成的。Node作为集群中的工作节点，运行真正的应用程序，在Node上Kubernetes管理的最小运行单元是Pod。Node上运行着Kubernetes的kubelet、kube-proxy服务进程，这些服务进程负责Pod的创建、启动、监控、重启、销毁，以及实现软件模式的负载均衡器。</p>
<p>传统IT系统中服务扩容和服务升级这两个难题，在Kubernetes集群中，只需为需要扩容的Service关联的Pod创建一个RC（Replication Controller），则该Service的扩容已至于后来的Service升级等问题就迎刃而解了。一个RC定义文件中包含3个关键信息。</p>
<ul>
<li>目标Pod的定义</li>
<li>副本数量（Replicas）</li>
<li>要监控的目录Pod的标签</li>
</ul>
<p>在创建好RC（系统将自动创建好Pod）后，Kubernetes会通过RC中定义的Label筛选出对应的Pod实例并实时监控其状态和数量，如果实例数量少于定义的副本数量（Replicas），则会根据RC中定义的Pod模板来创建一个新的Pod，然后将此Pod调度到合适的Node上启动运行，直到Pod实例的数量达到预定目标</p>
<h2 id="为什么要用kubernetes">为什么要用Kubernetes</h2>
<p>使用Kubernetes就是在全面拥抱微服务架构。微服务架构的核心是将一个巨大的单体应用分解为很多小的互相连接的微服务，一个微服务背后可能有多个实例副本在支撑，副本的数量可能随着系统的负荷变化而进行调整，内嵌的负载均衡器在这里发挥了重要作用。</p>
<p>Kubernetes系统架构具备了超强的横向扩容能力。</p>
<h2 id="使用minikube启动一个kubernetes集群">使用Minikube启动一个Kubernetes集群</h2>
<p>安装并启动：</p>
<pre><code># curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.2.0/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; mv minikube /usr/local/bin/
# minikube start --vm-driver=none --registry-mirror=https://registry.docker-cn.com
</code></pre>
<h2 id="安装kubernetes单机版">安装Kubernetes（单机版）</h2>
<p>（1）关闭CentOS自带的防火墙服务：</p>
<pre><code># systemctl disable firewalld
# systemctl stop firewalld
</code></pre>
<p>（2）安装etcd和Kubernetes软件：</p>
<pre><code>yum install -y etcd kubernetes
</code></pre>
<p>（3）按照顺序启动所有的服务：</p>
<pre><code># systemctl start etcd
# systemctl start docker
# systemctl start kube-apiserver
# systemctl start kube-controller-manager
# systemctl start kube-scheduler
# systemctl start kubelet
# systemctl start kube-proxy
</code></pre>
<h3 id="启动mysql服务">启动MySQL服务</h3>
<p>首先为MySQL服务创建一个RC定义文件：mysql-rc.yaml。</p>
<pre><code>apiVersion: v1
kind: ReplicationController                         # 副本控制器RC
metadata:
  name: mysql
spec:
  replicas: 1                                       # Pod副本期待数量
  selector:
    app: mysql                                      # 符合目标的Pod拥有此标签
  template:                                         # 根据此模板创建Pod的副本（实例）
    metadata:
      labels:
        app: mysql                                  # Pod副本拥有的标签，对应RC的Selector
    spec:          
      containers:                                   # Pod内容器的定义部分
      - name: mysql                                 # 容器的名称
        image: mysql                                # 容器对应的Docker Image
        ports: 
        - containerPort: 3306                       # 容器应用监听的端口号
        env:
        - name: MYSQL_ROOT_PASSWORD                 
          value: &quot;123456&quot;
</code></pre>
<p>yaml定义文件中的kind属性，用来表明此资源对象的类型，比如这里的值为“ReplicationController”，表示这是一个RC；spec是RC的相关属性定义，其中spec.replicas代表运行Pod实例的个数，spec.selector是RC的Pod标签（Label）选择器，即监控和管理拥有这些标签的Pod实例，确保当前集群上始终有且仅有replicas个Pod实例在运行。当集群中运行的Pod数量小于replicas时，RC会根据spec.template一节中定义的Pod模板生成新的Pod实例，spec.template.metadata.labels指定了该Pod的标签，需要特别注意的是：这里的labels必须匹配之前的spec.selector，否则此RC每次创建了一个无法匹配Label的Pod，就会不停地尝试创建新的Pod。</p>
<p>创建好mysql-rc.yaml文件以后，为了将它发布到Kubernetes集群中，我们在Master节点执行命令：</p>
<pre><code># kubectl create -f mysql-rc.yaml
replicationcontroller &quot;mysql&quot; created
# kubectl get rc            查看刚才创建的RC
NAME        DESIRED     CURRENT     AGE
mysql       1           1           1m
# kubectl get pods          查看Pod的创建情况
</code></pre>
<p>创建一个与之关联的Kubernetes Service， MySQL的定义文件（mysql-svc.yaml）：</p>
<pre><code>apiVersion: v1
kind: Service                               # 表明是Kubernetes Service
metadata:
  name: mysql                               # Service的全局唯一名称
spec:
  ports:
    - port: 3306                            # Service提供服务的端口号
  selector:                                 # Service对应的Pod拥有这里定义的标签
    app: mysql
</code></pre>
<p>其中，metadata.name是Service的服务名（ServiceName）；port属性则定义了Service的虚端口；spec.selector确定了哪些Pod副本（实例）对应到本服务。我们通过kubectl create命令创建Service对象。</p>
<pre><code># kubectl create -f mysql-svc.yaml
service &quot;mysql&quot; created
# kubectl get svc                           查看刚刚创建的service
NAME        CLUSTER-IP      EXTERNAL-IP     PORT(S)     AGE
mysql       168.169.253.144 &lt;none&gt;          3306/TCP    45s
</code></pre>
<p>168.169.253.144是Cluster IP地址，这是一个虚地址，Kubernetes集群中其他新创建的Pod就可以通过Service的Cluster IP+端口号3306来连接和访问。</p>
<p>在通常情况下，Cluster IP是在Service创建后由Kubernetes系统自动分配的，其他Pod无法预先知道某个Service的Cluster IP地址，因此需要一个服务发现机制来找到这个服务。为此，Kubernetes巧妙地使用了Linux环境变量来解决这个问题。我们只需知道，根据Service的唯一名字，容器可以从环境变量中获取到Service对应的Cluster IP地址和端口，从而发起了TCP/IP连接请求。</p>
<h3 id="启动tomcat应用">启动Tomcat应用</h3>
<p>创建对应的RC文件myweb-rc.yaml：</p>
<pre><code>apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb
spec:
  replicas: 2
  selector:
    app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      containers:
        - name: myweb
          image: kubeguide/tomcat-app:v1
          ports:
          - containerPort: 8080

</code></pre>
<p>创建对应的Service</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: myweb
spec:
  type: NodePort
  ports:
    - port: 8080
      nodePort: 30001
  selector:
    app: myweb
</code></pre>
<p>type=NodePort和nodePort=30001的两个属性，表明此Service开启了NodePort方式的外网访问模式，在Kubernetes集群之外，比如在本机的浏览器里，可以通过30001这个端口访问myweb</p>
<h2 id="kubernetes基本概念和术语">Kubernetes基本概念和术语</h2>
<h3 id="master">Master</h3>
<p>Kubernetes里的Master指的是集群控制节点，每个Kubernetes集群里需要有一个Master节点来负责整个集群的管理和控制，基本上Kubernetes的所有控制命令都发给它，它来负责具体的执行过程。Master节点通常会占据一个独立的服务器（高可用部署建议用3台服务器）。</p>
<p>Master节点上运行着以下一组关键进程：</p>
<ul>
<li>Kubernetes API Server（kube-apiserver）：提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程；</li>
<li>Kubernetes Controller Manager（kube-controller-manager）：Kubernetes里所有资源对象的自动化控制中心；</li>
<li>Kubernetes Scheduler（kube-scheduler）：负责资源调度（Pod调度）的进程。</li>
</ul>
<p>在Master节点上还需要启动一个etcd服务，因为Kubernetes里的所有资源对象的数据全部是保存在etcd中。</p>
<h3 id="node">Node</h3>
<p>除了Master，Kubernetes集群中的其他机器被称为Node节点。Node节点才是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上去。<br>
每个Node节点上都运行着以下一组关键进程：</p>
<ul>
<li>kubelet：负责Pod对应的容器的创建、启停等任务，同时与Master节点密切协作，实现集群管理的基本功能；</li>
<li>kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件；</li>
<li>Docker Engine（docker）：Docker引擎，负责本机的容器创建和管理工作。</li>
</ul>
<p>Node节点可以在运行期间动态增加到Kubernetes集群中，前提是这个节点已经正确安装、配置和启动了上述关键进程，在默认情况下kubelet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。kubelet进程会定时向Master节点汇报自身的情况，如操作系统、Docker版本、机器的CPU和内存情况，以及当前有哪些Pod在运行等，这样Master可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。</p>
<p>查看集群中有多少个Node：</p>
<pre><code># kubectl get nodes
NAME                STATUS      AGE
kubernetes-minion1  Ready       2d
</code></pre>
<p>通过kubectl describe node &lt;node_name&gt;来查看某个Node的详细信息：</p>
<pre><code># kubectl describe node kubernetes-minion1
</code></pre>
<ul>
<li>Node基本信息：名称、标签、创建时间；</li>
<li>Node当前运行状态，Node启动以后会做一系列的自检工作，比如磁盘是否满了，如果满了就标注OutOfDisk=True，否则继续检查内存是否不足（如果内存不足，就标注MemoryPressure=True），最后一切正常，就设置为Ready状态（Ready=True），该状态表示Node处于健康状态，Master就可以在其上调度新的任务了；</li>
<li>Node的主机地址与主机名；</li>
<li>Node上的资源总量：描述Node可用的系统资源，包括CPU、内存数量、最大可调度Pod数量等；</li>
<li>Node可分配资源量：描述Node当前可用于分配的资源量；</li>
<li>主机系统信息：包括主机的唯一标识UUID、Linux kernel版本号、操作系统类型与版本、Kubernetes版本号、kubelet与kube-proxy的版本号等。</li>
<li>当前正在运行的Pod列表概要信息；</li>
<li>已分配的资源使用概要信息；</li>
<li>Node相关的Event信息。</li>
</ul>
<h3 id="pod">Pod</h3>
<p>Pod是Kubernetes的最重要也最基本的概念，每个Pod都有一个特殊被称为“根容器”的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。</p>
<p>引入业务无关且不易死亡的Pause容器作为Pod的根容器，以它的状态代表整个容器组的状态，Pod里的多个业务容器共享Pause容器的IP，共享Pause容器挂载的Volume。</p>
<p>Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。Kubernetes要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术来实现，例如：Flannel、Open vSwitch等，在Kubernetes里，一个Pod里的容器与另外主机上的Pod容器能够直接通信。</p>
<p>Pod有两种类型：普通的Pod与静态Pod（Static Pod），后者不存放在Kubernetes的etcd存储里，而是存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动运行。而普通的Pod一旦被创建，就会被放入到etcd中存储，随后会被Kubernetes Master调度到某个具体的Node上并进行绑定，随后该Pod被对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动起来。当Pod里的某个容器停止时，Kubernetes会自动检测到这个问题并且重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机了，则会将这个Node上的所有Pod重新调度到其他节点上。</p>
<p>Kubernetes里的所有资源对象都可以采用yaml或者JSON格式的文件来定义或描述：</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myweb
  labels:
    name: myweb
spec:
  containers:
  - name: myweb
    image: kubeguide/tomcat-app:v1
    ports:
    - containerPort: 8080
    env:
    - name: MYSQL_SERVICE_HOST
      value: 'mysql'
    - name: MYSQL_SERVICE_PORT
      value: '3306'
</code></pre>
<p>Kind为Pod表明这是一个Pod的定义，metadata里的name属性为Pod的名字，metadata里还能定义资源对象的标签（Label），这里声明的myweb拥有一个name=myweb的标签（Label）。Pod里所包含的容器组的定义则在spec一节中声明，这里定义了一个名字为myweb、对应镜像为kubeguide/tomcat-app:v1的容器，该容器注入名为MYSQL_SERVICE_HOST='mysql'和MYSQL_SERVICE_PORT='3306'的环境变量，并且在8080端口上启动容器进程。Pod的IP加上这里的容器端口（containerPort）就组成一个新的概念--Endpoint，它代表着此Pod里的一个服务进程的对外通信地址。一个Pod也存在着具有多个Endpoint的情况，比如当把Tomcat定义为一个Pod时，可以对外暴露管理端口与服务端口这两个Endpoint</p>
<p>每个Pod都可以对其能使用的服务器上的计算资源设置限额，当前可以设置限额的计算资源有CPU和Memory两种，其中CPU的资源单位为CPU（Core）的数量，是一个绝对值而非相对值。一个CPU的配额对于绝大多数容器来说是相当大的资源配额，在Kubernetes里，通常以千分之一的CPU配额为最小单位，用m来表示。通常一个容器的CPU配额被定义为100<sub>300m，即占用0.1</sub>0.3个CPU。由于CPU配额是一个绝对值，所以无论在拥有一个Core的机器上，还是在拥有48个Core的机器上，100m这个配额所代表的的CPU的使用量都是一样的。与CPU配额类似，Memory配额也是一个绝对值，它的单位是内存字节数。</p>
<p>在Kubernetes里，一个计算资源进行配额限定需要设定以下两个参数。</p>
<ul>
<li>Requests：该资源的最小申请量，系统必须满足要求。</li>
<li>Limits：该资源最大允许使用的量，不能被突破，当容器试图使用超过这个量的资源时，可能会被Kubernetes Kill并重启。</li>
</ul>
<p>通常我们会把Requests设置为一个比较小的数值，符合容器平时的工作负载情况下的资源需求，而把Limit设置为峰值负载情况下资源占用的最大量。</p>
<pre><code>spec:
  containers:
  - name: db
    image: mysql
    resources:
      requests:
        memory: &quot;64Mi&quot;
        cpu: &quot;250m&quot;
      limits:
        memory: &quot;128Mi&quot;
        cpu: &quot;500m&quot;
</code></pre>
<h3 id="label标签">Label（标签）</h3>
<p>Label是Kubernetes系统中另外一个核心概念。一个Label是一个key=value的键值对，其中key与value由用户自己指定。Label可以附加到各种资源对象上，比如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。我们可以通过给指定的资源对象捆绑一个或多个不同的Label来实现多维度的资源分组管理功能，以便于灵活、方便地进行资源分配、调度、配置、部署等管理工作。常用的Label如下：</p>
<pre><code>版本标签：&quot;release&quot;: &quot;stable&quot;, &quot;release&quot;: &quot;canary&quot;...
环境标签：&quot;environment&quot;: &quot;dev&quot;, &quot;environment&quot;: &quot;qa&quot;, &quot;environment&quot;: &quot;production&quot;...
架构标签：&quot;tier&quot;: &quot;frontend&quot;, &quot;tier&quot;: &quot;backend&quot;, &quot;tier&quot;: &quot;middleware&quot;...
分区标签：&quot;partition&quot;: &quot;customerA&quot;, &quot;partition&quot;: &quot;customerB&quot;...
质量管控标签：&quot;track&quot;: &quot;daily&quot;, &quot;track&quot;: &quot;weekly&quot;...
</code></pre>
<p>Label相当于我们熟悉的“标签”，给某个资源对象定义一个Label，就相当于给它打了一个标签，随后可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实现了类似SQL的简单又通用的对象查询机制。</p>
<p>Label Selector可以被类比为SQL语句中的where查询条件，例如：name=redis-slave这个Label Selector作用于Pod时，可以被类比为select * from pod where pod’s name=‘redis-slave’这样的语句。</p>
<p>Label Selector在Kubernetes中的重要使用场景：</p>
<ul>
<li>kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本的数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程。</li>
<li>kube-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立起每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制。</li>
<li>通过对某些Node定义特定的Label，并且在Pod定义文件中使用NodeSelector这种标签调度策略，kube-scheduler进程可以实现Pod“定向调度”的特性。</li>
</ul>
<h3 id="replication-controller">Replication Controller</h3>
<p>RC是Kubernetes系统中的核心概念之一，简单来说就是定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，所以RC的定义包括如下几个部分。</p>
<ul>
<li>Pod期待的副本数（replicas）</li>
<li>用于筛选目标Pod的Label Selector</li>
<li>当Pod的副本数量小于预期数量时，用于创建新Pod的Pod模板（template）<br>
下面是一个完整的RC定义的例子，即确保拥有tier=frontend标签的这个Pod（运行Tomcat）容器在整个Kubernetes集群中始终只有一个副本：</li>
</ul>
<pre><code>apiVersion: v1
kind: ReplicationController
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    tier: frontend
  template:
    metadata:
      labels:
        app: app-demo
        tier: frontend
      spec:
        containers:
        - name: tomcat-demo
          image: tomcat
          imagePullPolicy: IfNotPresent
          env:
          - name: GET_HOSTS_FROM
            value: dns
          ports:
          - containerPort: 80
</code></pre>
<p>当我们定义了一个RC并提交到Kubernetes集群中以后，Master节点上的Controller Manager组件就得到通知，定期巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RC的期望值，如果有过多的Pod副本在运行，系统就会停掉一些Pod，否则系统就会再自动创建一些Pod。可以说，通过RC，Kubernetes实现了用户应用集群的高可用性，并且大大减少了系统管理员在传统IT环境中的手动运维操作。</p>
<p>在运行时，我们可以通过修改RC的副本数量，来实现Pod的动态缩放（Scaling）功能：</p>
<pre><code># kubectl scale rc redis-slave --replicas=3
scaled
</code></pre>
<p>删除RC并不会影响通过该RC已创建好的Pod，要想删除所有Pod，可以设置replicas的值为0，然后更新该RC。</p>
<p>由于Replication Controller与Kubernetes代码中模块Replication Controller同名，Kubernetes v1.2中升级为Replica Set，与RC唯一区别是：Replica Sets支持基于集合的Label selector（Set-based selector），而RC只支持基于等式的Label Selector（equality-based selector），这使得Replica Set的功能更强。</p>
<pre><code>apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: frontend
spec:
  selector:
    matchLabels:
      tier: frontend
    matchExpressions:
      - {key: tier, operator: In, values: [frontend]}
    template:
    ......
</code></pre>
<p>RC（Replica Set）的一些特性与作用：</p>
<ul>
<li>在大多数情况下，我们通过定义一个RC实现Pod的创建过程及副本数量的自动控制。</li>
<li>RC里包括完整的Pod定义模板。</li>
<li>RC通过Label Selector机制实现对Pod副本的自动控制。</li>
<li>通过改变RC里的Pod副本数量，可以实现Pod的扩容或缩容功能。</li>
<li>通过改变RC里Pod模板中的镜像版本，可以实现Pod的滚动升级功能。</li>
</ul>
<h3 id="deployment">Deployment</h3>
<p>Deployment是Kubernetes v1.2引入的新概念，引入的目的是为了更好地解决Pod的编排问题。Deployment在内部使用了Replica Set来实现目的，无论从Deployment的作用与目的、它的YAML定义，还是从它的具体命令行操作来看，我们都可以把它看作RC的一次升级，两者的相似度超过90%。</p>
<ul>
<li>创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。</li>
<li>检查Deployment的状态来看部署动作是否完成（Pod副本的数量是否达到预期的值）。</li>
<li>更新Deployment以创建新的Pod（比如镜像升级）。</li>
<li>暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后再恢复Deployment，进行新的发布。</li>
<li>扩展Deployment以应对高负载。</li>
<li>查看Deployment的状态，以此作为发布是否成功的指标。</li>
<li>清理不再需要的旧版本ReplicaSets。</li>
</ul>
<p>创建一个名为tomcat-deployment.yaml的Deployment的描述文件：</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      tier: frontend
    matchExpressions:
      - {key: tier, operator: In, values: [frontend]}
  template:
    metadata:
      labels:
        app: app-demo
        tier: frontend
    spec:
      containers:
      - name: tomcat-dmeo
        image: tomcat
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
</code></pre>
<p>运行命令创建Deployment：</p>
<pre><code># kubectl create -f tomcat-deployment.yaml
deployment &quot;tomcat-deploy&quot; created
# kubectl get deployments               查看Deployment的信息
NAME            DESIRED     CURRENT     UP-TO-DATE      AVAILABLE       AGE
tomcat-deploy   1           1           1               1               4m
</code></pre>
<ul>
<li>DESIRED：Pod副本数量的期望值，即Deployment里定义的Replica。</li>
<li>CURRENT：当前Replica的值，实际上是Deployment所创建的Replica Set里的Replica值，这个值不断增加，直到达到DESIRED为止，表明整个部署过程完成。</li>
<li>UP-TO-DATE：最新版本的Pod的副本数量，用于指示在滚动升级的过程中，有多少个Pod副本已经成功升级。</li>
<li>AVAILABEL：当前集群中可用的Pod副本数量，即集群中当前存活的Pod数量。</li>
</ul>
<h3 id="horizontal-pod-autoscalingpod横向自动扩容hpa">Horizontal Pod Autoscaling（Pod横向自动扩容，HPA）</h3>
<p>HPA与之前的RC、Deployment一样，也属于一种Kubernetes资源对象。通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。HPA可以有以下两种方式作为Pod负载的度量指标。</p>
<ul>
<li>CPUUtilizationPercentage。</li>
<li>应用程序自定义的度量指标，比如服务在每秒内的相应的请求数（TPS或QPS）</li>
</ul>
<p>CPUUtilizationPercentage是一个算术平均值，即目标Pod所有副本自身的CPU利用率的平均值。一个Pod自身的CPU利用率是该Pod当前CPU的使用量除以它的Pod Request的值，比如我们定义一个Pod的Pod Request为0.4，而当前Pod的CPU使用量为0.2，则它的CPU使用率为50%，如此一来，我们就可以算出来一个RC控制的所有Pod副本的CPU利用率的算术平均值。如果某一时刻CPUUtilizationPercentage的值超过80%，则意味着当前Pod副本数可能不足以支撑接下来更多的请求，需要进行动态扩容，而当请求高峰时段过去后，Pod的CPU利用率又会降下来，此时对应的Pod副本数应该自动减少到一个合理的水平。</p>
<pre><code>apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    kind: Deployment
    name: php-apache
  targetCPUUtilizationPercentage: 80
</code></pre>
<p>我们可以通过kubectl create命令创建一个HPA资源对象，还能通过下面的简单命令直接创建等价的HPA对象：</p>
<pre><code># kubectl autoscale deployment php-apache --cpu-percent=80 --min=1 --max=10
</code></pre>
<h3 id="service">Service</h3>
<p>Service也是Kubernetes里的最核心的资源对象之一，Kubernetes里的每个Service其实就是微服务架构中的一个“微服务”，Kubernetes的Service定义一个服务的访问入口地址，前端的应用（Pod）通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与Pod副本集群之间通过Label Selector来实现无缝对接的。RC的作用实际上是保证Service的服务能力和服务质量始终处于预期的标准。</p>
<p>通过分析、识别并建模系统中的所有服务为微服务--Kubernetes Service，最终我们的系统由多个提供不同业务能力而又彼此的微服务单元所组成，服务之间通过TCP/IP进行通信，从而形成了我们强大而又灵活的弹性网格，拥有了强大的分布式能力、弹性扩展能力、容错能力。</p>
<p>既然每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint（Pod IP+ContainerPort）以被客户端访问，现在多个Pod副本组成了一个集群来提供服务，那么客户端如何来访问它们呢？一般的做法是部署一个负载均衡器，为这组Pod开启一个对外的服务端口如8000，并且将这些Pod的Endpoint列表加入8000端口的转发列表中，客户端就可以通过负载均衡器的对外IP地址+服务端口来访问此服务，而客户端的请求最后被转发到哪个Pod，由负载均衡器的算法所决定。</p>
<p>Kubernetes提供的kube-proxy进程是一个智能的负载均衡器，它负责把对Service的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。Service不是公用一个负载均衡器的IP地址，而是每个Service分配了一个全局唯一的虚拟IP地址，这个虚拟IP被称为Cluster IP。这样每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成最基础的TCP通信。</p>
<p>定义一个名为tomcat-service的Service，服务端口为8080，拥有tier=frontend这个Label的所有Pod实例都属于它：</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port: 8080
  selector:
    tier: frontend
</code></pre>
<p>使用kubectl create -f tomcat-server.yaml运行这个Service，注意到tomcat-deployment.yaml里定义的Tomcat的Pod刚好拥有这个标签，所以我们刚才创建的tomcat-service已经对应到了一个Pod实例，运行下面的命令可以查看tomcat-service的Endpoint列表，其中172.17.1.3是Pod的IP地址，端口8080是Container暴露的端口：</p>
<pre><code># kubectl get endpoints
NAME                ENDPOINTS               AGE
kubernetes          192.168.18.131:6443     15d
tomcat-service      172.17.1.3              1m
# kubectl get svc tomcat-service -o yaml            获取tomcat-service分配的Cluster IP及更多的信息
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: ...
  name: tomcat-service
  namespace: default
  resourceVersion: &quot;23964&quot;
  selfLink: /api/v1/namespaces/default/services/tomcat-service
  uid: ...
spec:
  clusterIP: 169.169.65.227         
  ...
  type: ClusterIP
status:
  loadBalancer: {}
</code></pre>
<p>Kubernetes Service支持多个Endpoint，在存在多个Endpoint的情况下，要求每个Endpoint定义一个名字来区分，下面是Tomcat多端口的Service定义样例：</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port: 800
    name: service-port
  - port: 8005
    name: shutdown-port
</code></pre>
<p>给多个端口命名方便Kubernetes的服务发现机制。首先每个Kubernetes中的Service都有一个唯一的Cluster IP及唯一的名字，而名字是由开发者自己定义的，部署时也没必要改变，所以完全可以固定在配置中。接下来的问题是如何通过Service的名字找到对应的Cluster IP？最早Kubernetes采用了Linux环境变量的方式解决这个问题，即每个Service生成一些对应的Linux环境变量（ENV），并在每个Pod的容器在启动时，自动注入这些环境变量。考虑到环境变量的方式获取Service的IP与端口的方式仍然不太方便，不够直观，后来Kubernetes通过Add-On增值包的方式引入了DNS系统，把服务名作为DNS域名，这样一来，程序就可以直接使用服务名来建立通信连接了。目前Kubernetes上的大部分应用都已经采用了DNS这些新兴的服务发现机制。</p>
<h4 id="外部系统访问service的问题">外部系统访问Service的问题</h4>
<p>Kubernetes提供三种IP：</p>
<ul>
<li>Node IP：Node节点的IP地址</li>
<li>Pod IP： Pod的IP地址</li>
<li>Cluster IP：Service的IP地址</li>
</ul>
<p>Node IP是Kubernetes集群中每个节点的物理网卡的IP地址，这是一个真实存在的物理网络，所有属于这个网络的服务器之间都能通过这个网络直接通信。Pod IP是每个Pod的IP地址，它是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络，Kubernetes要求位于不同Node上的Pod可以彼此直接通信，所以Kubernetes里一个Pod里的容器访问另外一个Pod里的容器，就是通过Pod IP所在的虚拟二层网络进行通信的，而真实的TCP/IP流量则是通过Node IP所在的物理网卡流出的。</p>
<p>Clustor IP也是一个虚拟IP，但更像是一个“伪造”的IP网络：</p>
<ul>
<li>Cluster IP仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配IP地址</li>
<li>Cluster IP无法被Ping，因为没有一个实体网络对象来响应</li>
<li>Cluster IP只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备TCP/IP通信的基础，并且它们属于Kubernetes集群这样的一个封闭的空间，集群之外的节点如果要访问这个通信端口，则需要做一些额外的工作</li>
<li>在Kubernetes集群之内，Node IP网、Pod IP网与Cluster IP网之间的通信，采用的是Kubernetes自己设计的一种编程方式的特殊的路由规则，与我们熟知的IP路由有很大不同</li>
</ul>
<p>Service的Cluster IP属于Kubernetes集群内部的地址，无法在集群外部直接使用这个地址，那么用户如何访问它呢？采用NodePort：</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 31002
  selector:
    tier: frontend
</code></pre>
<p>其中，nodePort: 31002这个属性表明我们手动指定tomcat-service的NodePort为31002，否则Kubernetes会自动分配一个可用的端口。NodePort的实现方式是在Kubernetes集群里的每个Node上为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要用任意一个Node的IP地址+具体的NodePort端口号即可访问此服务。但是NodePort还没有完全解决外部访问Service的所有问题，比如负载均衡问题，假如我们的集群中有10个Node，此时最好有一个负载均衡器（如Load balancer组件），外部的请求只需访问此负载均衡器的IP地址，由负载均衡器负责转发流量到后面某个Node的NodePort上。</p>
<p>Load balancer组件独立于Kubernetes集群之外，通常是一个硬件的负载均衡器，或者是以软件方式实现的HAProxy或者Nginx。对于每个Service，我们通常需要配置一个对应的Load balancer实例来转发流量到后端的Node上，这的确增加了工作量及出错的概率。于是Kubernetes提供了自动化的解决方案，把Service的type=NodePort改为type=LoadBalancer，此时Kubernetes会自动陈佳颖对应的Load balancer实例并返回它的IP地址供外部客户端使用。</p>
<h3 id="volume存储卷">Volume（存储卷）</h3>
<p>Volume是Pod中能够被多个容器访问的共享目录。Kubernetes中的Volume定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或重启时，Volume中的数据也不会丢失。Kubernetes支持多种类型的Volume，比如GlusterFS、Ceph等先进的分布式系统。</p>
<p>我们给之前的Tomcat Pod增加一个名字为datavol的Volume，并且Mount到容器的/mydata-data目录上：</p>
<pre><code>template:
  metadata:
    labels:
      app: app-demo
      tier: frontend
  spec:
    volumes:
      - name: datavol
        emptyDir: {}
    containers:
    - name: tomcat-demo
      image: tomcat
      volumeMounts:
        - mountPath: /mydata-data
          name: datavol
      imagePullPolicy: IfNotPresent
</code></pre>
<h4 id="emptydir">emptyDir</h4>
<p>一个emptyDir Volume是在Pod分配到Node时创建的。它的初始内容为空，并且无需指定宿主机上对应的目录文件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也会被永久删除。emptyDir的一些用途：</p>
<ul>
<li>临时空间</li>
<li>长时间任务的中间过程CheckPoint的临时保存目录</li>
<li>一个容器需要从另一个容器中获取数据的目录（多容器共享目录）</li>
</ul>
<h4 id="hostpath">hostPath</h4>
<p>hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以下几个方面：</p>
<ul>
<li>容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统进行存储</li>
<li>需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统。</li>
</ul>
<h4 id="gcepersistentdisk">gcePersistentDisk</h4>
<p>使用这种类型的Volume表示使用谷歌共有云提供的永久磁盘存放Volume的数据</p>
<h4 id="awselasticblockstore">awsElasticBlockStore</h4>
<p>该类型的Volume使用亚马逊公有云提供的EBS Volume存储数据。</p>
<p>其他类型的Volume：NFS、iscsi、flocker、rbd、glusterfs等</p>
<h3 id="namespace命名空间">Namespace（命名空间）</h3>
<p>Namespace是Kubernetes系统中的另一个非常重要的概念，Namespace在很多情况下用于实现多租户的资源隔离。Namespace通过将集群内部的资源对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。</p>
<p>Kubernetes集群在启动后，会创建一个名为“default”的Namespace，通过kubectl可以查看到：</p>
<pre><code># kubectl get namespaces
NAME        LABELS      STATUS
default     &lt;none&gt;      Active
</code></pre>
<p>如果用户创建的Pod、RC、Service不特别指定Namespace，都将被系统创建到这个默认的名为default的Namespace中。Namespace的定义很简单：</p>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: development
</code></pre>
<p>一旦创建了Namespace，我们在创建资源对象时就可以指定这个资源对象属于哪个Namespace。比如在下面的例子中，我们定义了一个名为busybox的Pod，放入development这个Namespace里：</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: development
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &quot;3600&quot;
    name: busybox
</code></pre>
<p>在kubectl命令中加入--namespace参数来查看某个命名空间中的对象：</p>
<pre><code># kubectl get pods --namespace=development
NAME        READY       STATUS      RESTARTS    AGE
busybox     1/1         Running     0           1m
</code></pre>
<p>当我们给每个租户创建一个Namespace来实现多租户的资源隔离时，还能结合Kubernetes的资源配额管理，限定不同租户能占用的资源，例如CPU使用量、内存使用量等。</p>
<h1 id="kubernetes实践指南">Kubernetes实践指南</h1>
<h2 id="kubernetes安装与配置">Kubernetes安装与配置</h2>
<p>本机使用Centos7.4，需要关闭防火墙服务：</p>
<pre><code># systemctl disable firewalld
# systemctl stop firewalld
# setenforce 0
</code></pre>
<h3 id="1安装kubeadm和相关工具">1.安装Kubeadm和相关工具</h3>
<p>添加文件/etc/yum.repos.d/kubernetes.repo，内容如下：</p>
<pre><code>[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
enable=1
</code></pre>
<p>然后yum install安装kubeadm和相关工具，启动Docker服务与kubelet服务，并设置开机启动：</p>
<pre><code># yum install -y docker kubelet kubeadm kubectl kubernetes-cni
# systemctl enable docker &amp;&amp; systemctl start docker 
# systemctl enable kubelet &amp;&amp; systemctl start kubelet
</code></pre>
<h3 id="2下载kubernetes的相关镜像">2.下载Kubernetes的相关镜像</h3>
<pre><code>[root@kubernetes-master ~]# docker pull warrior/pause-amd64:3.0
Trying to pull repository docker.io/warrior/pause-amd64 ... 
3.0: Pulling from docker.io/warrior/pause-amd64
a3ed95caeb02: Pull complete 
ce150f7a21ec: Pull complete 
Digest: sha256:35e1d4e39ad85c8ec163d0741882cc6442233385d914766a0db77e9e18776b90
Status: Downloaded newer image for docker.io/warrior/pause-amd64:3.0
[root@kubernetes-master ~]# docker tag warrior/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0
[root@kubernetes-master ~]# docker pull warrior/etcd-amd64:3.0.17
Trying to pull repository docker.io/warrior/etcd-amd64 ... 
3.0.17: Pulling from docker.io/warrior/etcd-amd64
9db6bc3045a3: Pull complete 
11f056d9da6e: Pull complete 
78d5c8d6ad80: Pull complete 
Digest: sha256:76cfdd5b54c3f4baec36c46a107a5fca321e2b9ebcb1ca8b09ff06c423558190
Status: Downloaded newer image for docker.io/warrior/etcd-amd64:3.0.17
[root@kubernetes-master ~]# 
[root@kubernetes-master ~]# docker tag warrior/etcd-amd64:3.0.17 gcr.io/google_containers/etcd-amd64:3.0.17
[root@kubernetes-master ~]# docker pull warrior/kube-apiserver-amd64:v1.6.0
Trying to pull repository docker.io/warrior/kube-apiserver-amd64 ... 
v1.6.0: Pulling from docker.io/warrior/kube-apiserver-amd64
bd4b2003aa95: Pull complete 
d6570e7cf860: Pull complete 
Digest: sha256:6b49095296d74f57e5c4971d5cbaa4f31099fe081aa32c14aaad6a1731fc43cb
Status: Downloaded newer image for docker.io/warrior/kube-apiserver-amd64:v1.6.0
[root@kubernetes-master ~]# docker tag warrior/kube-apiserver-amd64:v1.6.0 gci.io/google_containers/kube-apiserver-amd64:v1.6.0
[root@kubernetes-master ~]# docker pull warrior/kube-scheduler-amd64:v1.6.0
Trying to pull repository docker.io/warrior/kube-scheduler-amd64 ... 
v1.6.0: Pulling from docker.io/warrior/kube-scheduler-amd64
bd4b2003aa95: Already exists 
3d0373d7af87: Pull complete 
Digest: sha256:727fd20c622bc20485501fceb856b7b7161cd20a71778d3292459e2acca670f7
Status: Downloaded newer image for docker.io/warrior/kube-scheduler-amd64:v1.6.0
[root@kubernetes-master ~]# docker tag warrior/kube-scheduler-amd64:v1.6.0 gcr.io/google_containers/kube-scheduler-amd64:v1.6.0
[root@kubernetes-master ~]# docker pull warrior/kube-controller-manager-amd64:v1.6.0
Trying to pull repository docker.io/warrior/kube-controller-manager-amd64 ... 
v1.6.0: Pulling from docker.io/warrior/kube-controller-manager-amd64
bd4b2003aa95: Already exists 
d0ac0a1d98e3: Pull complete 
Digest: sha256:3528d32313a23e12b6e28cf10eec50b69f652ce2dd8abde621da45088e133aab
Status: Downloaded newer image for docker.io/warrior/kube-controller-manager-amd64:v1.6.0
[root@kubernetes-master ~]# docker tag warrior/kube-controller-manager-amd64:v1.6.0 gcr.io/google_containers/kube-controller-manager-amd64:v1.6.0
[root@kubernetes-master ~]# docker pull warrior/kube-proxy-amd64:v1.6.0
Trying to pull repository docker.io/warrior/kube-proxy-amd64 ... 
v1.6.0: Pulling from docker.io/warrior/kube-proxy-amd64
eed9f2fe5754: Pull complete 
6ac89e3b5f3b: Pull complete 
8d4ff2906840: Pull complete 
Digest: sha256:4b6e4c9b5800c7156cb2a6ca296891123ba8cede7667c494e3ed76ba8ef0f94b
Status: Downloaded newer image for docker.io/warrior/kube-proxy-amd64:v1.6.0
[root@kubernetes-master ~]# docker tag warrior/kube-proxy-amd64:v1.6.0 gcr.io/google_containers/kube-proxy-amd64:v1.6.0
[root@kubernetes-master ~]# docker pull gysan/dnsmasq-metrics-amd64:1.0
Trying to pull repository docker.io/gysan/dnsmasq-metrics-amd64 ... 
1.0: Pulling from docker.io/gysan/dnsmasq-metrics-amd64
c0cb142e4345: Pull complete 
a3ed95caeb02: Pull complete 
08935cb095f1: Pull complete 
Digest: sha256:69f62c72d8423677d508779ef37dc4afb1cde2072044d7d931c2f303032290c9
Status: Downloaded newer image for docker.io/gysan/dnsmasq-metrics-amd64:1.0
[root@kubernetes-master ~]# docker tag gysan/dnsmasq-metrics-amd64:1.0 gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
[root@kubernetes-master ~]# docker pull warrior/k8s-dns-kube-dns-amd64:1.14.1
Trying to pull repository docker.io/warrior/k8s-dns-kube-dns-amd64 ... 
1.14.1: Pulling from docker.io/warrior/k8s-dns-kube-dns-amd64
e5e3de7014a6: Pull complete 
5e96979410c8: Pull complete 
Digest: sha256:f2d9698f64586b74e6eba4c423974101d5c34d10844a3f1e5e24716e0e6f3e6b
Status: Downloaded newer image for docker.io/warrior/k8s-dns-kube-dns-amd64:1.14.1
[root@kubernetes-master ~]# docker tag warrior/k8s-dns-kube-dns-amd64:1.14.1 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
[root@kubernetes-master ~]# docker pull warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1
Trying to pull repository docker.io/warrior/k8s-dns-dnsmasq-nanny-amd64 ... 
1.14.1: Pulling from docker.io/warrior/k8s-dns-dnsmasq-nanny-amd64
d1426d011624: Pull complete 
b7721aa8d47c: Pull complete 
4a8eae141ba6: Pull complete 
d643c5d0fc0c: Pull complete 
7f3be923ec71: Pull complete 
Digest: sha256:1807899b0d07015d0b1e700d43859c657362b6455ff30bd7a3e14879afcc0ce3
Status: Downloaded newer image for docker.io/warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1
[root@kubernetes-master ~]# docker tag warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
[root@kubernetes-master ~]# docker pull warrior/k8s-dns-sidecar-amd64:1.14.1
Trying to pull repository docker.io/warrior/k8s-dns-sidecar-amd64 ... 
1.14.1: Pulling from docker.io/warrior/k8s-dns-sidecar-amd64
e5e3de7014a6: Already exists 
1fc5171ce36b: Pull complete 
Digest: sha256:20b89a3d369b119258a712e80cac62050ec41a6bdcb3ea1f31619747cdd7168f
Status: Downloaded newer image for docker.io/warrior/k8s-dns-sidecar-amd64:1.14.1
[root@kubernetes-master ~]# docker tag warrior/k8s-dns-sidecar-amd64:1.14.1 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
[root@kubernetes-master ~]# docker pull awa305/kube-discovery-amd64:1.0
Trying to pull repository docker.io/awa305/kube-discovery-amd64 ... 
1.0: Pulling from docker.io/awa305/kube-discovery-amd64
627d84b6655e: Pull complete 
dddb11175374: Pull complete 
a3ed95caeb02: Pull complete 
Digest: sha256:7b725840d232709846b634e24bd481e7a045c03da65e89ca3705911e5be45b55
Status: Downloaded newer image for docker.io/awa305/kube-discovery-amd64:1.0
[root@kubernetes-master ~]# docker tag awa305/kube-discovery-amd64:1.0 gcr.io/google_containers/kube-discovery-amd64:1.0
[root@kubernetes-master ~]# docker pull gysan/exechealthz-amd64:1.2
Trying to pull repository docker.io/gysan/exechealthz-amd64 ... 
1.2: Pulling from docker.io/gysan/exechealthz-amd64
8ddc19f16526: Pull complete 
a3ed95caeb02: Pull complete 
7d1ee54af137: Pull complete 
476d09449781: Pull complete 
Digest: sha256:018e247cd52a6eb02c06e9c3e88534beb957e48f3d0b768df50de93943bfbac2
Status: Downloaded newer image for docker.io/gysan/exechealthz-amd64:1.2
[root@kubernetes-master ~]# docker tag gysan/exechealthz-amd64:1.2 gcr.io/google_containers/exechealthz-amd64:1.2
</code></pre>
<h3 id="3运行kubeadm-init安装master">3.运行kubeadm init安装Master</h3>
<p>准备工作就绪，执行kubeadm init命令即可一键完成Kubernetes Master节点的安装</p>
<pre><code># kubeadm init --kubernetes-version=1.15.1      
[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.15.2: output: Trying to pull repository k8s.gcr.io/kube-controller-manager ... 
Get https://k8s.gcr.io/v1/_ping: dial tcp 64.233.189.82:443: i/o timeout

# cat kubernetes.sh                 如果出现以上错误，则需要手动编写脚本拉取镜像然后再执行kubeadm init
docker pull mirrorgooglecontainers/kube-apiserver:v1.15.1
docker pull mirrorgooglecontainers/kube-controller-manager:v1.15.1
docker pull mirrorgooglecontainers/kube-scheduler:v1.15.1
docker pull mirrorgooglecontainers/kube-proxy:v1.15.1
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd:3.3.10
docker pull coredns/coredns:1.3.1

docker tag mirrorgooglecontainers/kube-proxy:v1.15.1  k8s.gcr.io/kube-proxy:v1.15.1
docker tag mirrorgooglecontainers/kube-scheduler:v1.15.1 k8s.gcr.io/kube-scheduler:v1.15.1
docker tag mirrorgooglecontainers/kube-apiserver:v1.15.1 k8s.gcr.io/kube-apiserver:v1.15.1
docker tag mirrorgooglecontainers/kube-controller-manager:v1.15.1 k8s.gcr.io/kube-controller-manager:v1.15.1
docker tag mirrorgooglecontainers/etcd:3.3.10  k8s.gcr.io/etcd:3.3.10
docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
docker tag mirrorgooglecontainers/pause:3.1  k8s.gcr.io/pause:3.1


docker rmi mirrorgooglecontainers/kube-apiserver:v1.15.1
docker rmi mirrorgooglecontainers/kube-controller-manager:v1.15.1
docker rmi mirrorgooglecontainers/kube-scheduler:v1.15.1
docker rmi mirrorgooglecontainers/kube-proxy:v1.15.1
docker rmi mirrorgooglecontainers/pause:3.1
docker rmi mirrorgooglecontainers/etcd:3.3.10
docker rmi coredns/coredns:1.3.1
# bash kubernetes.sh
# kubeadm init --kubernetes-version=1.15.1      出现以下结果，表示安装成功
Your Kubernetes control-plane has initialized successfully!
......
kubeadm join 172.16.31.105:6443 --token tbjzb3.834myq2mm0g7aewm \
    --discovery-token-ca-cert-hash sha256:f0cecec6a64fa51d533abc64db1207d4d8d6c626445c233a66800b4d79222e33 

</code></pre>
<h3 id="4安装node加入集群">4.安装Node，加入集群</h3>
<p>在新的节点安装kubeadm和相关工具</p>
<pre><code># yum install -y docker kubelet kubeadm kubectl kubernetes-cni
# systemctl enable docker &amp;&amp; systemctl start docker 
# systemctl enable kubelet &amp;&amp; systemctl start kubelet
</code></pre>
<p>执行kubeadm join命令加入集群：</p>
<pre><code># kubeadm join --token tbjzb3.834myq2mm0g7aewm 172.16.31.105:6443
</code></pre>
<p>其中token的值来自使用kubeadm安装Master过程中提示的最后一行提供的token，172.16.31.105:6443是Master的ip和端口地址。默认情况下Master节点不参与工作负载，如果希望Master节点成为一个Node节点：</p>
<pre><code># kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre>
<h3 id="5安装网络插件">5.安装网络插件</h3>
<p>通过kubectl get nodes命令，发现Kubernetes提示Master节点为NotReady状态，这是因为还没有安装CNI网络插件：</p>
<pre><code>[root@kubernetes-master ~]# kubectl get nodes
NAME                STATUS     ROLES    AGE   VERSION
kubernetes-master   NotReady   master   26m   v1.15.1
</code></pre>
<p>安装weave网络插件：</p>
<pre><code># kubectl apply -f https://git.io/weave-kube-1.6
</code></pre>
<h3 id="6验证kubernetes集群安装完成">6.验证Kubernetes集群安装完成</h3>
<p>执行下面的命令，验证Kubernetes集群的相关Pod是否都正常创建并运行：</p>
<pre><code>[root@kubernetes-master ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                        READY   STATUS    RESTARTS   AGE
kube-system   coredns-5c98db65d4-c7kwc                    1/1     Running   0          31m
kube-system   coredns-5c98db65d4-h998m                    1/1     Running   0          31m
kube-system   etcd-kubernetes-master                      1/1     Running   0          30m
kube-system   kube-apiserver-kubernetes-master            1/1     Running   0          30m
kube-system   kube-controller-manager-kubernetes-master   1/1     Running   0          30m
kube-system   kube-proxy-58jhz                            1/1     Running   0          31m
kube-system   kube-scheduler-kubernetes-master            1/1     Running   0          30m
kube-system   weave-net-plfgc                             2/2     Running   0          96s
</code></pre>
<p>至此，通过kubeadm工具就实现了Kubernetes集群的快速搭建，如果安装失败，可以执行kubeadm reset命令将主机恢复原状。</p>
<h2 id="以二进制文件方式安装kubernetes集群">以二进制文件方式安装Kubernetes集群</h2>
<p>下载压缩包：</p>
<pre><code>[root@kubernetes-master ~]# wget https://github.com/kubernetes/kubernetes/archive/v1.15.2.tar.gz
</code></pre>
<h2 id="kubectl命令行用法">Kubectl命令行用法</h2>
<p>kubectl作为客户端cli工具，可以让用户通过命令行的方式对Kubernetes集群进行操作。</p>
<h3 id="kubectl用法概述">kubectl用法概述</h3>
<p>kubectl命令行的语法如下：</p>
<pre><code># kubectl [command] [TYPE] [NAME] [flags]
1.command：子命令，用于操作Kubernetes集群资源对象的命令，例如create、delete、describe、get、apply等。
2.TYPE：资源对象的类型，区分大小写，能以单数形式、复数形式或者简写形式表示。
3.NAME：资源对象的名称，区分大小写，如果不指定名称，则系统将返回属于TYPE的全部对象的列表，例如# kubectl get pods将返回所有Pod的列表。
4.flags：kubectl子命令的可选参数，例如使用“-s”指定apiserver的URL地址而不用默认值。
</code></pre>
<h3 id="操作示例">操作示例</h3>
<h4 id="1创建资源对象">1.创建资源对象</h4>
<p>根据yaml配置文件一次性创建service和rc：</p>
<pre><code># kubectl create -f my-service.yaml -f my-rc.yaml
</code></pre>
<p>根据<directory>目录下所有.yaml、.yml、.json文件的定义进行创建操作：</p>
<pre><code># kubectl create -f &lt;directory&gt;
</code></pre>
<h4 id="2查看资源对象">2.查看资源对象</h4>
<p>查看所有Pod列表：</p>
<pre><code># kubectl get pods
</code></pre>
<p>查看rc和service列表：</p>
<pre><code>kubectl get rc,service
</code></pre>
<h4 id="3描述资源对象">3.描述资源对象</h4>
<p>查看Node的详细信息：</p>
<pre><code># kubectl describe nodes &lt;node-name&gt;
</code></pre>
<p>显示Pod的详细信息：</p>
<pre><code># kubectl describe pods/&lt;pod-name&gt;
</code></pre>
<p>显示由RC管理的Pod信息：</p>
<pre><code># kubectl describe pods &lt;rc-name&gt;
</code></pre>
<h4 id="4删除资源对象">4.删除资源对象</h4>
<p>基于pod.yaml定义的名称删除Pod：</p>
<pre><code># kubectl delete -f pod.yaml
</code></pre>
<p>删除所有包含某个label的Pod和service：</p>
<pre><code># kubectl delete pods,services -l name=&lt;label-name&gt;
</code></pre>
<p>删除所有Pod：</p>
<pre><code># kubectl delete pods --all
</code></pre>
<h4 id="5执行容器的命令">5.执行容器的命令</h4>
<p>执行Pod的date命令，默认使用Pod中的第1个容器执行：</p>
<pre><code># kubectl exec &lt;pod-name&gt; date
</code></pre>
<p>指定Pod中某个容器执行date命令：</p>
<pre><code># kubectl exec &lt;pod-name&gt; -c &lt;container-name&gt; date
</code></pre>
<p>通过bash获得Pod中某个容器的TTY，相当于登录容器：</p>
<pre><code># kubectl exec -ti &lt;pod-name&gt; -c &lt;container-name&gt; /bin/bash
</code></pre>
<h4 id="6查看容器的日志">6.查看容器的日志</h4>
<p>查看容器输出到stdout的日志：</p>
<pre><code># kubectl logs &lt;pod-name&gt;
</code></pre>
<p>跟踪查看容器的日志，相当于tail -f命令的结果：</p>
<pre><code># kubectl logs -f &lt;pod-name&gt; -c &lt;container-name&gt;
</code></pre>
<h2 id="深入掌握pod">深入掌握Pod</h2>
<h3 id="pod定义详情">Pod定义详情</h3>
<p>yaml格式的Pod定义文件的完整内容如下：</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: string
  namespace: string
  labels:
    - name: stirng
  annotations:
    - name: string
spec:
  containers:
  - name: string
    image: string
    imagePullPolicy: [Always | Never | IfNotPresent]
    command: [string]
    args: [string]
    workingDir: string
    volumeMounts:
    - name: string
      mountPath: string
      readOnly: boolean
    ports:
    - name: string
      containerPort: int
      hostPort: int
      protocol: string
    env:
    - name: string
      value: string
    resources:
      limits:
        cpu: string
        memory: string
      requests:
        cpu: string
        memory: string
    livenessProbe:
      exec:
        command: [string]
      httpGet:
        path: string
        port: number
        host: string
        scheme: string
        httpHeaders:
        - name: string
          value: string
      tcpSocket:
        port: number
      initialDelaySeconds: 0
      timeoutSeconds: 0
      periodSeconds: 0
      successThreshold: 0
      failureThreshold: 0
    securityContext:
      privileged: false
  restartPolicy: [Always | Never | OnFailure]
  nodeSelector: object
  imagePullSecrets:
  - name: string
  hostNetwork: false
  volumes:
  - name: string
    emptyDir: {}
    hostPath:
      path: string
    secret:
      secretName: string
      items:
      - key: string
        path: string
    configMap:
      name: string
      items:
      - key: string
        path: string
</code></pre>
<h2 id="深入掌握service">深入掌握Service</h2>
<h1 id="kubernetes核心原理">Kubernetes核心原理</h1>
<p>Kubernetes API Server的核心功能是提供了Kubernetes各类资源对象（Pod、RC、Service等）的增、删、改、查及Watch等HTTP Rest接口，成为集群内各个功能模块直接数据交互和通信的中心枢纽。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea Demo]]></title>
        <id>https://liuhuipy.github.io/post/hello-gridea/</id>
        <link href="https://liuhuipy.github.io/post/hello-gridea/">
        </link>
        <updated>2019-01-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>